{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c3093e-ba6f-46f6-ac68-159c08a1cdd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=\"\", # your OpenAI api key\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae82a0fb-878f-4b24-bc91-a60482d6b22d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "\n",
    "\n",
    "def extract_scores(output_text):\n",
    "    scores = re.findall(r'-?\\d+\\.\\d+', output_text)\n",
    "    if len(scores) >= 2:\n",
    "        us_score, cn_score = map(float, scores[:2])\n",
    "        return us_score, cn_score\n",
    "    else:\n",
    "        return None, None\n",
    "\n",
    "def analyze_sentiment(row, model_name, lang):\n",
    "\n",
    "    try:\n",
    "\n",
    "        if lang == 'CN':\n",
    "\n",
    "            title = row['label_3']\n",
    "            comment = row['text_cleaned']\n",
    "            ip_label = row['ip_label']\n",
    "            \n",
    "            completion = client.chat.completions.create(\n",
    "                model = model_name, \n",
    "                top_p = 0.1,\n",
    "                presence_penalty = -2.0,\n",
    "    \n",
    "\n",
    "                messages = [\n",
    "                    {\n",
    "                        \"role\": \"system\",\n",
    "                        \"content\": f\"你是一位文本情感分析师，你需要给文本的情感分类。\\\n",
    "                                     请将评论分为3类，分别是：1.仇恨、攻击性言论以及批评和抱怨，2.相对中立的评论，3.支持和认可。\\\n",
    "                                     你必须在回答的开头回复两个分类（分别代表对美国的分类和对中国的分类），并在后面给出相应的分析过程。 \\\n",
    "                                     对于无法判断指向美国还是中国的评论，请先尝试分析指向的对象，如果难以判断，请给出相对中立范围内的分类。请尽可能不要给出完全中立的分类。\",\n",
    "                    },\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": f\"现在你需要对来自中国的短视频平台上的评论进行情感分析。 \\\n",
    "                                     此条短视频题目是：<text>{title}</text>，此条评论的内容是：<text>{comment}</text>，评论者的ip地址为<text>{ip_label}</text>。 \\\n",
    "                                     请根据以上信息，并结合美国和中国的现实情况，仔细揣摩评论在对应的视频下指向的对象和蕴含的信息，并考虑可能存在的反讽的情况， \\\n",
    "                                     分析这条短视频评论对美国和中国的整体国内社会，包括但不限于政策、企业、生活等方面的支持程度。 \\\n",
    "                                     请回复两个分类，分别代表对美国的分类和对中国的分类。\",\n",
    "                    },\n",
    "    \n",
    "                ]           \n",
    "            )\n",
    "            \n",
    "            output_text = completion.choices[0].message.content\n",
    "            us_score, cn_score = extract_scores(output_text)    \n",
    "            return pd.Series([output_text, us_score, cn_score])\n",
    "            \n",
    "        elif lang == 'EN':\n",
    "\n",
    "            title = row['title']\n",
    "            comment = row['text_cleaned']\n",
    "    \n",
    "            completion = client.chat.completions.create(\n",
    "                model = model_name,  \n",
    "                top_p = 0.1,\n",
    "                presence_penalty = -2.0,\n",
    "\n",
    "                # stop = [\"The\" ,\"Analysis\"],\n",
    "    \n",
    "                messages = [\n",
    "                    {\n",
    "                        \"role\": \"system\",\n",
    "                        \"content\": f\"You are a sentiment analysis expert, and your task is to assign sentiment category to a given text. \\\n",
    "                                    Please categorize comments into 3 categories: 1. Hate, offensive speech and criticism and complaints, 2. Relatively neutral comments, 3. Support and approval.\\\n",
    "                                    You must provide two categories at the beginning of your response (one representing sentiment towards the U.S. and the other towards China), followed by a detailed analysis of the reasoning behind these categories. \\\n",
    "                                    For comments where it's unclear whether the sentiment is directed towards the U.S. or China, try to infer the target of the comment. If it's too ambiguous to determine, assign categories within the neutral range. Please avoid giving a fully neutral category whenever possible.\"\n",
    "                    },\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": f\"You are now required to perform sentiment analysis on a comment from an American short video platform. \\\n",
    "                                    The title of the video is: <text>{title}</text>, and the content of the comment is: <text>{comment}</text>. The commenter’s account is registered in the U.S.  \\\n",
    "                                    Based on this information, and considering the current situations in both the U.S. and China, \\\n",
    "                                    carefully analyze the underlying meaning of the comment, including any potential sarcasm, irony, or hidden comparisons. \\\n",
    "                                    For example, a comment that appears to praise another country might be indirectly criticizing domestic issues in the U.S. or China. \\\n",
    "                                    Please analyze the sentiment of this comment toward the overall domestic situation in both the U.S. and China, including but not limited to policies, culture, businesses, and living conditions. \\\n",
    "                                    Respond with two scores, representing the level of support for the U.S. and China, respectively.\"\n",
    "                    },\n",
    "                ]\n",
    "            \n",
    "            )\n",
    "            \n",
    "            output_text = completion.choices[0].message.content\n",
    "            us_score, cn_score = extract_scores(output_text)\n",
    "            return pd.Series([output_text, us_score, cn_score])\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing row {row.name}: {e}\")\n",
    "        return pd.Series([None, None, None])\n",
    "\n",
    "\n",
    "\n",
    "def handle_failed_rows(df_output, model_name, output_file, chunk_size, start_row, end_row, csv_file, lang):\n",
    "    try:\n",
    "        failed_rows = df_output[df_output[[f'LLM_analysis_{model_name}', f'US_score_{model_name}', f'CN_score_{model_name}']].isnull().any(axis=1)]\n",
    "        \n",
    "        if start_row is not None and end_row is not None:\n",
    "            failed_rows = failed_rows[(failed_rows['row_id'] >= start_row) & (failed_rows['row_id'] <= end_row)]\n",
    "\n",
    "        if not failed_rows.empty:\n",
    "            print(f\"Found {len(failed_rows)} failed rows in range {start_row} to {end_row}. Reprocessing them...\")\n",
    "\n",
    "            for idx in tqdm(range(0, len(failed_rows), chunk_size), desc=\"Reprocessing failed rows\"):\n",
    "                chunk = failed_rows.iloc[idx:idx + chunk_size]\n",
    "\n",
    "                row_ids = chunk['row_id'].values\n",
    "                original_rows = pd.read_csv(csv_file)\n",
    "                \n",
    "                original_rows = original_rows.loc[original_rows.index.isin(row_ids - 1)]  # -1 是因为 row_id 从1开始\n",
    "                \n",
    "                result_chunk = original_rows.apply(lambda row: analyze_sentiment(row, model_name, lang), axis=1)\n",
    "\n",
    "                if len(result_chunk) == len(chunk):\n",
    "                    df_output.loc[df_output['row_id'].isin(row_ids), [f'LLM_analysis_{model_name}', f'US_score_{model_name}', f'CN_score_{model_name}']] = result_chunk.values\n",
    "                else:\n",
    "                    print(f\"Error: Result length {len(result_chunk)} does not match chunk length {len(chunk)}\")\n",
    "                    continue\n",
    "\n",
    "            df_output.to_csv(output_file, index=False, encoding='utf-8-sig')\n",
    "        else:\n",
    "            print(f\"No failed rows found in range {start_row} to {end_row}.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error reprocessing failed rows: {e}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def process(csv_file, output_file, model_name, lang, chunk_size=10, start_row=None, end_row=None):\n",
    "    try:\n",
    "        df = pd.read_csv(csv_file)\n",
    "\n",
    "        if lang not in ['CN', 'EN']:\n",
    "            raise ValueError(\"variable 'lang' must be 'CN' or 'EN'\")\n",
    "        \n",
    "        \n",
    "        if lang == 'CN':\n",
    "            df['label_3'] = df['label_3'].str.replace('#', '', regex=False)  \n",
    "        \n",
    "        elif lang == 'EN':\n",
    "            df['title'] = df['title'].str.replace('#', '', regex=False)\n",
    "        \n",
    "\n",
    "        df['row_id'] = df.index + 1  \n",
    "        \n",
    "        if start_row is not None and end_row is not None:\n",
    "            df = df.iloc[start_row-1:end_row]  \n",
    "            print(f\"Processing rows from {start_row} to {end_row}.\")\n",
    "        \n",
    "        try:\n",
    "            df_output = pd.read_csv(output_file)\n",
    "            if 'row_id' not in df_output.columns:\n",
    "                print(\"Warning: Output file does not contain 'row_id'. Cannot accurately check processed rows.\")\n",
    "                processed_row_ids = set()\n",
    "            else:\n",
    "                processed_row_ids = set(df_output['row_id'])\n",
    "            print(f\"Loaded {len(processed_row_ids)} already processed rows.\")\n",
    "        \n",
    "        except FileNotFoundError:\n",
    "            df_output = pd.DataFrame(columns=['row_id', f'LLM_analysis_{model_name}', \n",
    "                                              f'US_score_{model_name}', \n",
    "                                              f'CN_score_{model_name}'])\n",
    "            processed_row_ids = set()\n",
    "\n",
    "        df_to_process = df[~df['row_id'].isin(processed_row_ids)]\n",
    "\n",
    "        for start_idx in tqdm(range(0, len(df_to_process), chunk_size), desc=\"Processing data\"):\n",
    "            chunk = df_to_process.iloc[start_idx:start_idx + chunk_size]\n",
    "            \n",
    "            result_chunk = chunk.apply(lambda row: analyze_sentiment(row, model_name, lang), axis=1)\n",
    "            \n",
    "            if len(result_chunk) == len(chunk):\n",
    "                chunk.loc[:, [f'LLM_analysis_{model_name}', f'US_score_{model_name}', f'CN_score_{model_name}']] = result_chunk.values\n",
    "            else:\n",
    "                print(f\"Error: Result length {len(result_chunk)} does not match chunk length {len(chunk)}\")\n",
    "                continue\n",
    "                \n",
    "            chunk_clean = chunk.dropna(how='all', axis=1)\n",
    "\n",
    "            chunk_clean = chunk.dropna(how='any', axis=0)\n",
    "\n",
    "            \n",
    "            df_output = pd.concat([df_output, chunk[['row_id', f'LLM_analysis_{model_name}', \n",
    "                                                     f'US_score_{model_name}', \n",
    "                                                     f'CN_score_{model_name}']]])\n",
    "            \n",
    "            df_output.to_csv(output_file, index=False, encoding='utf-8-sig')\n",
    "\n",
    "        handle_failed_rows(df_output, model_name, output_file, chunk_size, start_row, end_row, csv_file, lang)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Processing failed: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97810087-93ed-4704-8710-1f393b0d0032",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables\n",
    "\n",
    "# csv_file: the read document\n",
    "# output_file: the written document\n",
    "# model_name: the model to use\n",
    "# chunk_size: how many rows to process in a batch\n",
    "# start_row: The first row to process (included) \n",
    "# end_row=None: Last row processed (included)\n",
    "\n",
    "process('labeled_CN_douyin.csv', 'labeled_CN_douyin_.csv', model_name = \"gpt-4o-mini\", lang='EN', chunk_size=10, start_row=1, end_row=1000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
