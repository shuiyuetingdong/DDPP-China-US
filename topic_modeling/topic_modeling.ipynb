{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a57057f-d25c-416d-a457-96fba73edf94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import jieba\n",
    "import cntext as ct\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import unicodedata\n",
    "\n",
    "\n",
    "from bertopic.representation import KeyBERTInspired, MaximalMarginalRelevance\n",
    "from bertopic import BERTopic\n",
    "from umap import UMAP\n",
    "from sklearn.decomposition import PCA\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from topictuner import TopicModelTuner as TMT\n",
    "\n",
    "\n",
    "nltk.download('stopwords') \n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "\n",
    "def remove_stopwords_en(text):\n",
    "    text = text.lower()\n",
    "    stop_words_pattern = r'\\b(' + '|'.join(re.escape(word) for word in stop_words) + r')\\b'\n",
    "    filtered_text = re.sub(stop_words_pattern, ' ', text)\n",
    "    filtered_text = re.sub(r'\\s+', ' ', filtered_text).strip()\n",
    "\n",
    "    return filtered_text\n",
    "\n",
    "\n",
    "\n",
    "stopwords_cn = ct.load_pkl_dict('STOPWORDS.pkl')['STOPWORDS']['chinese'] \n",
    "\n",
    "def clean_text(text): \n",
    "    words = jieba.lcut(text)\n",
    "    words = [w for w in words if w not in stopwords_cn]\n",
    "    return ' '.join(words)\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "def extract_and_remove_emojis(text):\n",
    "    emojis = re.findall(r'\\[.*?\\]', text)\n",
    "    cleaned_text = re.sub(r'\\[.*?\\]', '', text)\n",
    "    emojis_str = ' '.join(emojis)\n",
    "    return cleaned_text, emojis_str\n",
    "\n",
    "def extract_at(text):\n",
    "    at = re.findall(r'@\\S+ ?', text)\n",
    "    cleaned_text = re.sub(r'@\\S+ ?', '', text)\n",
    "    at_str = ' '.join(at)\n",
    "    return cleaned_text, at_str\n",
    "\n",
    "def remove_meaningless_haha(text): \n",
    "    haha = re.findall(r'哈{2,}(?!姐)[^\\w]*', text)\n",
    "\n",
    "    cleaned_text = re.sub(r'哈{2,}(?!姐)[^\\w]*', '', text)\n",
    "    \n",
    "    haha_str = ' '.join(haha)\n",
    "    return cleaned_text, haha_str\n",
    "\n",
    "def remove_meaningless_haha_en(text):\n",
    "    return re.sub(r'(ha){2,}', '', text)\n",
    "\n",
    "\n",
    "punctuation_to_remove = r'[（）()：:\\[\\]【】「」|\\/…·—_!！?？，“”\"]'\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    return re.sub(punctuation_to_remove, '', text)\n",
    "\n",
    "# Below are the preprocessing function in English\n",
    "\n",
    "def extract_emojis_and_symbols(text):\n",
    "    emojis_and_symbols = re.findall(r'[\\U0001F600-\\U0001F64F\\U0001F300-\\U0001F5FF\\U0001F680-\\U0001F6FF\\U0001F700-\\U0001F77F\\U0001F900-\\U0001F9FF\\U0001FA00-\\U0001FA6F\\U00002700-\\U000027BF]+|[\\U0001F1E6-\\U0001F1FF]{2}', text)\n",
    "\n",
    "    cleaned_text = re.sub(r'[\\U0001F600-\\U0001F64F\\U0001F300-\\U0001F5FF\\U0001F680-\\U0001F6FF\\U0001F700-\\U0001F77F\\U0001F900-\\U0001F9FF\\U0001FA00-\\U0001FA6F\\U00002700-\\U000027BF]+|[\\U0001F1E6-\\U0001F1FF]{2}', '', text)\n",
    "\n",
    "    emojis_and_symbols_str = ' '.join(emojis_and_symbols)\n",
    "    return cleaned_text, emojis_and_symbols_str\n",
    "\n",
    "def remove_fyp(text):\n",
    "    pattern = r'\\bfyp\\b|\\bforyou\\b|\\bforyoupage\\b|\\bfypシ\\b|\\bfypツ\\b'\n",
    "    return re.sub(pattern, '', text, flags=re.IGNORECASE).strip()\n",
    "\n",
    "\n",
    "def remove_fyp_strings(text):\n",
    "    pattern_to_remove = r'\\b(fyp|foryou)[a-zA-Z]*\\s*'\n",
    "\n",
    "    return re.sub(pattern_to_remove, '', text)\n",
    "\n",
    "def remove_tiktok(text):\n",
    "    pattern = r'\\b(抖音|tiktok)\\b(?!chinesetiktok)'\n",
    "    return re.sub(pattern, '', text).strip()\n",
    "\n",
    "def remove_china(text):\n",
    "    pattern = r'(中国|\\bchina\\b)'\n",
    "    return re.sub(pattern, '', text, flags=re.IGNORECASE).strip()\n",
    "\n",
    "\n",
    "\n",
    "def remove_invalid_words(text):\n",
    "    words = re.findall(r'\\b\\w+\\b|\\b[\\u4e00-\\u9fff]+\\b', text)\n",
    "    \n",
    "    def is_valid_word(word):\n",
    "        for char in word:\n",
    "            if re.match(r'[\\u4e00-\\u9fff]', char):\n",
    "                continue\n",
    "            if char.isalpha() or char.isdigit():\n",
    "                if 'LATIN' in unicodedata.name(char, '') and unicodedata.category(char) in ['Lu', 'Ll', 'Nd']:\n",
    "                    if not ('WITH' in unicodedata.name(char, '')):\n",
    "                        continue\n",
    "            return False\n",
    "        return True\n",
    "    \n",
    "    valid_words = [word for word in words if is_valid_word(word)]\n",
    "    \n",
    "    return ' '.join(valid_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11a2201-a82f-4c71-999a-d7a304475c6f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Preprocessing Chinese comments\n",
    "\n",
    "df = pd.read_csv('douyin_CN_relevant_UTF8.csv').astype(str)\n",
    "\n",
    "# df = pd.read_csv('cnews.csv').astype(str)\n",
    "\n",
    "#print(df['comment'].head(10))\n",
    "\n",
    "df['text_cleaned'], df['emojis'] = zip(*df['comment'].apply(extract_and_remove_emojis)) # emoji\n",
    "df['text_cleaned'], df['at'] = zip(*df['text_cleaned'].apply(extract_at)) # @\n",
    "df['text_cleaned'], df['haha'] = zip(*df['text_cleaned'].apply(remove_meaningless_haha)) # 哈哈哈\n",
    "df['text_cleaned'] = df['text_cleaned'].apply(clean_text) # stopwords\n",
    "df['text_cleaned'] = df['text_cleaned'].apply(lambda x: x.strip() if isinstance(x, str) and x.strip() == '' else x) \n",
    "\n",
    "\n",
    "\n",
    "df.replace('nan', pd.NA, inplace=True)\n",
    "\n",
    "df['text_cleaned'].replace('', pd.NA, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "df_cleaned = df.dropna(subset=['text_cleaned'], how='any')\n",
    "\n",
    "\n",
    "print(df_cleaned['text_cleaned'].head(302))\n",
    "print(df_cleaned['haha'].head(302))\n",
    "\n",
    "\n",
    "docs = df_cleaned['text_cleaned'].tolist() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e6ae4e7-a90f-46a0-b212-ed73adbe52dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing English comments\n",
    "\n",
    "df = pd.read_csv('filtered_tiktok_US_30w.csv').astype(str)\n",
    "\n",
    "\n",
    "print(df['comment'].head(20))\n",
    "\n",
    "df['text_cleaned'], df['emojis_and_symbols'] = zip(*df['comment'].apply(extract_emojis_and_symbols)) # emoji\n",
    "df['text_cleaned'], df['at'] = zip(*df['text_cleaned'].apply(extract_at)) # @\n",
    "\n",
    "df['text_cleaned'] = df['text_cleaned'].apply(remove_fyp)\n",
    "df['text_cleaned'] = df['text_cleaned'].apply(remove_fyp_strings)\n",
    "df['text_cleaned'] = df['text_cleaned'].apply(remove_tiktok)\n",
    "df['text_cleaned'] = df['text_cleaned'].apply(remove_china)\n",
    "# df['text_cleaned'] = df['text_cleaned'].str.replace('#', ' ', regex=False)  # \"#\"\n",
    "df['text_cleaned'], df['haha'] = zip(*df['text_cleaned'].apply(remove_meaningless_haha)) # 哈哈哈\n",
    "df['text_cleaned'] = df['text_cleaned'].apply(remove_meaningless_haha_en) # hahaha\n",
    "\n",
    "df['text_cleaned'] = df['text_cleaned'].apply(remove_punctuation) \n",
    "\n",
    "df['text_cleaned'] = df['text_cleaned'].apply(remove_invalid_words) # only English and Chinese\n",
    "\n",
    "df['text_cleaned'] = df['text_cleaned'].apply(clean_text) \n",
    "\n",
    "df['text_cleaned'] = df['text_cleaned'].apply(remove_stopwords_en) \n",
    "\n",
    "df['text_cleaned'] = df['text_cleaned'].apply(lambda x: x.strip() if isinstance(x, str) and x.strip() == '' else x) \n",
    "\n",
    "\n",
    "df.replace('nan', pd.NA, inplace=True)\n",
    "\n",
    "df['text_cleaned'].replace('', pd.NA, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "df_cleaned = df.dropna(subset=['text_cleaned'], how='any')\n",
    "\n",
    "print(df_cleaned['text_cleaned'].head(20))\n",
    "\n",
    "\n",
    "docs = df_cleaned['text_cleaned'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec37ca85-0be2-4221-a2fa-10255cd79915",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bertopic.representation import KeyBERTInspired, MaximalMarginalRelevance\n",
    "from bertopic import BERTopic\n",
    "from umap import UMAP\n",
    "from sklearn.decomposition import PCA\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from topictuner import TopicModelTuner as TMT\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "sentence_model = SentenceTransformer(\"/root/paraphrase-multilingual-MiniLM-L12-v2-main\") # English\n",
    "\n",
    "#sentence_model = SentenceTransformer(\"/root/m3e-base\") # Chinese\n",
    "\n",
    "embeddings = sentence_model.encode(docs, show_progress_bar=True)\n",
    "\n",
    "\n",
    "def rescale(x, inplace=False):\n",
    "    \"\"\" Rescale an embedding so optimization will not have convergence issues.\n",
    "    \"\"\"\n",
    "    if not inplace:\n",
    "        x = np.array(x, copy=True)\n",
    "\n",
    "    x /= np.std(x[:, 0]) * 10000\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "# Initialize and rescale PCA embeddings\n",
    "pca_embeddings = rescale(PCA(n_components=5).fit_transform(embeddings))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a480f4-c07d-4e99-8fda-a58ca0bf7946",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hdbscan import HDBSCAN\n",
    "\n",
    "\n",
    "# representation_model = KeyBERTInspired()\n",
    "representation_model = MaximalMarginalRelevance(diversity=0.2)\n",
    "\n",
    "# Start UMAP from PCA embeddings\n",
    "umap_model = UMAP(\n",
    "    n_neighbors=30,\n",
    "    n_components=5,\n",
    "    min_dist=0.0,\n",
    "    metric=\"cosine\",\n",
    "    init=pca_embeddings,\n",
    "    angular_rp_forest=True\n",
    ")\n",
    "\n",
    "# from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# vectorizer_model = CountVectorizer()  \n",
    "\n",
    "hdbscan_model = HDBSCAN(min_cluster_size=300, min_samples=30)\n",
    "\n",
    "\n",
    "\n",
    "topic_model = BERTopic(\n",
    "                       hdbscan_model=hdbscan_model, # If TMT is used, comment out the parameter\n",
    "                       representation_model=representation_model,\n",
    "                       embedding_model=sentence_model,\n",
    "                       #embedding_model=\"/root/m3e-base\",\n",
    "                       umap_model=umap_model,\n",
    "                       #vectorizer_model=vectorizer_model,\n",
    "                       verbose=True,\n",
    "                       low_memory=True,\n",
    "                       calculate_probabilities=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a4dc017-caeb-41c0-bbbd-8c9f571b567b",
   "metadata": {},
   "outputs": [],
   "source": [
    "topics, probs = topic_model.fit_transform(docs, embeddings) \n",
    "#topic_model.get_topic_info()\n",
    "new_topics = topic_model.reduce_outliers(docs, topics) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e83b1f4d-91d2-45af-8ef0-44f2037ae29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.update_topics(docs, topics=new_topics)\n",
    "\n",
    "topic_model.save(\"/topic_modeling/model_name\") # save your model\n",
    "\n",
    "# topic_model = BERTopic.load(\"/topic_modeling/model_name\") # load your model\n",
    "\n",
    "topic_model.get_topic_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "882fb2d4-7fde-453d-b4ad-c015426ce596",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_info = topic_model.get_topic_info()\n",
    "\n",
    "topic_info.to_csv('tiktok_comment_topic_modeling2.csv', index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e65bb9-ec7b-4b2e-8114-f5d07bf37d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To tune parameters using TMT, run the following blocks\n",
    "\n",
    "from topictuner import TopicModelTuner as TMT\n",
    "tuned_model = TMT.wrapBERTopicModel(topic_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd95d2d-5c01-46ce-9a1a-d5d1656819d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuned_model.embeddings = embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "031907db-90c4-47e1-882d-0f8a75352801",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuned_model.reduce()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ac2ed3-d40c-45b2-8529-18bbf0cf1123",
   "metadata": {},
   "outputs": [],
   "source": [
    "lastRunResultsDF = tuned_model.randomSearch([*range(100,1200)], [.1, .25, .5, .75, 1]) \n",
    "lastRunResultsDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac7ed473-b32d-4f1f-8ffd-24c6d93cb555",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lastRunResultsDF = tuned_model.pseudoGridSearch([*range(100,1000)], [x/100 for x in range(10,101,10)]) \n",
    "tuned_model.summarizeResults(lastRunResultsDF).sort_values(by=['number_uncategorized'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a83ac2fa-0542-44a9-8286-b419ee854acf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tuned_model.save('tuned_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15359de3-7a67-436c-84bf-efbb9a0b7d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuned_model = TMT.load('tuned_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b6fb291-d58d-4561-84e5-d8a8850dbc09",
   "metadata": {},
   "outputs": [],
   "source": [
    "bt1 = tuned_model.getBERTopicModel(100,10) # Fill in min_cluster_size and min_samples with the best results measured above\n",
    "topics, probs = bt1.fit_transform(docs, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e16f06-b8d8-4428-96e0-c3ef34b5eb66",
   "metadata": {},
   "outputs": [],
   "source": [
    "bt1.get_topic_info()\n",
    "\n",
    "topic_info.to_csv('topic_info_latest.csv', index=False, encoding='utf-8')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
